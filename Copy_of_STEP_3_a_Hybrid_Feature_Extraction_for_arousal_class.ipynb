{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasparagjyoti/EEG-Channel-Selection-for-Emotion-Recognition/blob/main/Copy_of_STEP_3_a_Hybrid_Feature_Extraction_for_arousal_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEeOQnx0DMQg"
      },
      "source": [
        "# 1. Importing python libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "K7lHqm_mHB5a",
        "outputId": "f9aefa87-61ee-48a9-8a39-cc03c77e0d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: antropy in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: stochastic in /usr/local/lib/python3.7/dist-packages (from antropy) (0.6.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from stochastic->antropy) (1.19.5)\n",
            "Requirement already satisfied: scipy<2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from stochastic->antropy) (1.4.1)\n",
            "Requirement already satisfied: hurst in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.7/dist-packages (from hurst) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hurst) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18->hurst) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18->hurst) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.18->hurst) (1.15.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.2->statsmodels) (1.15.0)\n",
            "Requirement already satisfied: scikit-plot in /usr/local/lib/python3.7/dist-packages (0.3.7)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.0.0)\n",
            "Requirement already satisfied: deap in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deap) (1.19.5)\n",
            "Requirement already satisfied: scoop in /usr/local/lib/python3.7/dist-packages (0.7.1.1)\n",
            "Requirement already satisfied: pyzmq>=13.1.0 in /usr/local/lib/python3.7/dist-packages (from scoop) (22.3.0)\n",
            "Requirement already satisfied: greenlet>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from scoop) (1.1.2)\n",
            "Collecting argparse>=1.1\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install antropy\n",
        "!pip install hurst\n",
        "!pip install statsmodels --upgrade\n",
        "!pip install scikit-plot\n",
        "!pip install deap\n",
        "!pip install scoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYHR9s0xCpiF"
      },
      "outputs": [],
      "source": [
        "import scipy, csv, statistics, pickle, math, random, numpy, joblib, pywt\n",
        "from __future__ import unicode_literals\n",
        "import itertools\n",
        "from math import factorial\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.signal as ss\n",
        "import scipy.stats as st\n",
        "import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.model_selection as model_selection\n",
        "import antropy as ent\n",
        "import statsmodels.api as sm\n",
        "from pywt import wavedec\n",
        "from time import time\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import stats, signal, interpolate\n",
        "from sklearn import metrics\n",
        "from scipy.integrate import simps\n",
        "from sklearn import svm, datasets, metrics, preprocessing\n",
        "from scipy.stats import entropy, kurtosis\n",
        "from scipy.signal import butter, sosfilt, sosfreqz, lfilter, find_peaks\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.cross_decomposition import CCA\n",
        "from math import log,e, floor\n",
        "from hurst import compute_Hc, random_walk\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "#from statsmodels.tsa.arima_model import ARIMA\n",
        "import statsmodels.tsa.arima.model as stats\n",
        "from scipy.misc import electrocardiogram\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, r2_score, confusion_matrix, classification_report, plot_confusion_matrix, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, cross_validate, learning_curve, train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from deap import creator, base, tools, algorithms\n",
        "from scoop import futures\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oNCiADSkzIC",
        "outputId": "4aeee109-8f9e-4d8d-8a5f-1a8cdf71c9ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, '/usr/local/lib/python3.7/dist-packages/antropy', ('', '', 5))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import imp\n",
        "imp.find_module('antropy')\n",
        "# change the hazard coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tllf9xaIJDVA"
      },
      "outputs": [],
      "source": [
        "# Sample rate and desired cutoff frequencies (in Hz).\n",
        "fs = 128\n",
        "lowcut = 0.5\n",
        "highcut = 45\n",
        "T = 60\n",
        "nsamples = T * fs\n",
        "t = np.linspace(0, T, nsamples, endpoint=False)\n",
        "a = 0.02\n",
        "f0 = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjI_tb5iJDLJ"
      },
      "outputs": [],
      "source": [
        "def butter_bandpass(lowcut, highcut, fs, order = 3):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype = 'band', analog = False)\n",
        "    return b, a\n",
        "def butter_bandpass_filter(time_series, lowcut, highcut, fs, order = 5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order = order)\n",
        "    y = lfilter(b, a, time_series)\n",
        "    return y\n",
        "def eye_movement_artifact(time_data): # parameter must be an 2D array like 32_channels*7860_data\n",
        "    time_data = time_data.transpose() # Inverse that 2D array\n",
        "    ica = FastICA()\n",
        "    comps = ica.fit_transform(time_data)\n",
        "    data_after = comps.transpose()  # Invert the array \n",
        "    return data_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a8vUnF3JDft"
      },
      "outputs": [],
      "source": [
        "def signal_pro(data):\n",
        "    mean_value = 0\n",
        "    # do the bandpass filter\n",
        "    for i in range(40):\n",
        "        for j in range(32):\n",
        "            data[i][j] = butter_bandpass_filter(data[i][j], lowcut, highcut, fs, order=5)\n",
        "    # creating dummy variable which contains same data information \n",
        "    error_eye =  np.zeros((40,32,7680))\n",
        "    new_data =  np.zeros((40,32,7680))\n",
        "    for i in range(40):\n",
        "        for j in range(32):\n",
        "            for k in range(7680):\n",
        "                error_eye[i][j][k] = data[i][j][k]\n",
        "                new_data[i][j][k] = data[i][j][k]\n",
        "    for i in range(40):\n",
        "        error_eye[i] = eye_movement_artifact(error_eye[i])\n",
        "    for i in range(40):\n",
        "        for j in range(32):\n",
        "            mean_value = np.mean(error_eye[i][j])\n",
        "            for k in range(7680):\n",
        "                if(data[i][j][k] > 0.0):\n",
        "                    # data is positive\n",
        "                    new_data[i][j][k] = data[i][j][k] - abs(mean_value)\n",
        "                else: # data is negative\n",
        "                    new_data[i][j][k] = data[i][j][k] + abs(mean_value)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6v5RNxaJDnR"
      },
      "outputs": [],
      "source": [
        "def energy(data): # The energy is the summation of a squared signal\n",
        "    energy_value = 0\n",
        "    energy_value = sum(abs(data)**2)\n",
        "    return energy_value\n",
        "def average_power(data):  # The average power is the signal mean square\n",
        "    avg_power = 0\n",
        "    avg_power = sum(abs(data)**2)\n",
        "    return (avg_power/data.shape[0])\n",
        "def first_difference(data):\n",
        "    temp = 0\n",
        "    fd = 0\n",
        "    for i in range(1,data.shape[0] - 1):\n",
        "        temp = abs(data[i+1]-data[i])\n",
        "        fd = fd + temp\n",
        "    return fd/data.shape[0]\n",
        "def second_difference(data):\n",
        "    temp, sd = 0, 0\n",
        "    for i in range(1,data.shape[0] - 2):\n",
        "        temp = abs(data[i+1]-data[i])\n",
        "        sd = sd + temp\n",
        "    return sd/data.shape[0]\n",
        "def katz_fractal_dimension(data, axis=-1):  # Katz Fractal Dimension\n",
        "    x = np.asarray(data)\n",
        "    dists = np.abs(np.diff(x, axis=axis))\n",
        "    ll = dists.sum(axis=axis)\n",
        "    ln = np.log10(ll / dists.mean(axis=axis))\n",
        "    aux_d = x - np.take(x, indices=[0], axis=axis)\n",
        "    d = np.max(np.abs(aux_d), axis=axis)\n",
        "    kfd = np.squeeze(ln / (ln + np.log10(d / ll)))\n",
        "    if not kfd.ndim:\n",
        "        kfd = kfd.item()\n",
        "    return kfd\n",
        "def non_linear_energy(data): # Nonlinear Energy\n",
        "    nle = 0\n",
        "    nle_value = 0 \n",
        "    for i in range(1,data.shape[0]-1):\n",
        "        nle = (data[i]**2)-(data[i+1]*data[i-1])\n",
        "        nle_value = nle + nle_value\n",
        "    return nle_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vv8MC30MlhP"
      },
      "outputs": [],
      "source": [
        "def shannon_entopy(data):     # Shannon Entropy\n",
        "    pd_series = pd.Series(data)\n",
        "    counts = pd_series.value_counts()\n",
        "    return entropy (counts)    # scipy.stats.entropy(counts) is called\n",
        "def _embed(x, order = 3, delay = 1):\n",
        "    N = len(x)\n",
        "    Y = np.empty((order, N - (order - 1) * delay))\n",
        "    for i in range(order):\n",
        "        Y[i] = x[i * delay:i * delay + Y.shape[1]]\n",
        "    return Y.T\n",
        "def util_pattern_space(time_series, lag, dim):\n",
        "    n = len(time_series)\n",
        "    if lag * dim > n:\n",
        "        raise Exception('Result matrix exceeded size limit, try to change lag or dim.')\n",
        "    elif lag < 1:\n",
        "        raise Exception('Lag should be greater or equal to 1.')\n",
        "    pattern_space = np.empty((n - lag * (dim - 1), dim))\n",
        "    for i in range(n - lag * (dim - 1)):\n",
        "        for j in range(dim):\n",
        "            pattern_space[i][j] = time_series[i + j * lag]\n",
        "    return pattern_space\n",
        "def util_standardize_signal(time_series):\n",
        "    return (time_series - np.mean(time_series)) / np.std(time_series)\n",
        "def util_granulate_time_series(time_series, scale):\n",
        "    n = len(time_series)\n",
        "    b = int(np.fix(n / scale))\n",
        "    temp = np.reshape(time_series[0:b*scale], (b, scale))\n",
        "    cts = np.mean(temp, axis = 1)\n",
        "    return cts\n",
        "def util_rolling_window(a, window):\n",
        "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
        "    strides = a.strides + (a.strides[-1],)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "def shannon_entropy_WPE(time_series):\n",
        "    if not isinstance(time_series, str): # Check if string\n",
        "        time_series = list(time_series)\n",
        "    data_set = list(set(time_series))  # Create a frequency data\n",
        "    freq_list = []\n",
        "    for entry in data_set:\n",
        "        counter = 0.\n",
        "        for i in time_series:\n",
        "            if i == entry:\n",
        "                counter += 1\n",
        "        freq_list.append(float(counter) / len(time_series))\n",
        "    ent = 0.0 # Shannon entropy\n",
        "    for freq in freq_list:\n",
        "        ent += freq * np.log2(freq)\n",
        "    ent = -ent\n",
        "    return ent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Mtnpo-NLJs"
      },
      "outputs": [],
      "source": [
        "def sample_entropy_WPE(time_series, sample_length = 7680, tolerance = None):\n",
        "    #The code below follows the sample length convention of Ref [1] so:\n",
        "    M = sample_length - 1;\n",
        "    time_series = np.array(time_series)\n",
        "    if tolerance is None:\n",
        "        tolerance = 0.1*np.std(time_series)\n",
        "    n = len(time_series)\n",
        "    #Ntemp is a vector that holds the number of matches. N[k] holds matches templates of length k\n",
        "    Ntemp = np.zeros(M + 2)\n",
        "    #Templates of length 0 matches by definition:\n",
        "    Ntemp[0] = n*(n - 1) / 2\n",
        "    for i in range(n - M - 1):\n",
        "        template = time_series[i:(i+M+1)];#We have 'M+1' elements in the template\n",
        "        rem_time_series = time_series[i+1:]\n",
        "        searchlist = np.nonzero(np.abs(rem_time_series - template[0]) < tolerance)[0]\n",
        "        go = len(searchlist) > 0;\n",
        "        length = 1;\n",
        "        Ntemp[length] += len(searchlist)\n",
        "        while go:\n",
        "            length += 1\n",
        "            nextindxlist = searchlist + 1;\n",
        "            nextindxlist = nextindxlist[nextindxlist < n - 1 - i]#Remove candidates too close to the end\n",
        "            nextcandidates = rem_time_series[nextindxlist]\n",
        "            hitlist = np.abs(nextcandidates - template[length-1]) < tolerance\n",
        "            searchlist = nextindxlist[hitlist]\n",
        "            Ntemp[length] += np.sum(hitlist)\n",
        "            go = any(hitlist) and length < M + 1\n",
        "    sampen =  - np.log(Ntemp[1:] / Ntemp[:-1])\n",
        "    return sampen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iEvD6pjNVGV"
      },
      "outputs": [],
      "source": [
        "def multiscale_entropy(time_series, sample_length =7680, tolerance = None, maxscale = None):\n",
        "    if tolerance is None:\n",
        "        #we need to fix the tolerance at this level. If it remains 'None' it will be changed in call to sample_entropy()\n",
        "        tolerance = 0.1*np.std(time_series)\n",
        "    if maxscale is None:\n",
        "        maxscale = len(time_series)\n",
        "    mse = np.zeros(maxscale)\n",
        "    for i in range(maxscale):\n",
        "        temp = util_granulate_time_series(time_series, i+1)\n",
        "        mse[i] = sample_entropy_WPE(temp, sample_length, tolerance)[-1]\n",
        "    return mse\n",
        "def permutation_entropy_WPE(time_series, order = 3, delay = 1, normalize = False):\n",
        "    x = np.array(time_series)\n",
        "    hashmult = np.power(order, np.arange(order))\n",
        "    # Embed x and sort the order of permutations\n",
        "    sorted_idx = _embed(x, order=order, delay=delay).argsort(kind='quicksort')\n",
        "    # Associate unique integer to each permutations\n",
        "    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n",
        "    # Return the counts\n",
        "    _, c = np.unique(hashval, return_counts=True)\n",
        "    # Use np.true_divide for Python 2 compatibility\n",
        "    p = np.true_divide(c, c.sum())\n",
        "    pe = -np.multiply(p, np.log2(p)).sum()\n",
        "    if normalize:\n",
        "        pe /= np.log2(factorial(order))\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngaWTvugNaVX"
      },
      "outputs": [],
      "source": [
        "def weighted_permutation_entropy(time_series, order=3, normalize=False):\n",
        "    x = np.array(time_series)\n",
        "    hashmult = np.power(order, np.arange(order))\n",
        "    embedded = _embed(x, order=order)\n",
        "    sorted_idx = embedded.argsort(kind='quicksort')\n",
        "    weights = np.var(util_rolling_window(x, order), 1)\n",
        "    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n",
        "    mapping = {}\n",
        "    for i in np.unique(hashval):\n",
        "        mapping[i] = np.where(hashval == i)[0] \n",
        "    weighted_counts = dict.fromkeys(mapping)\n",
        "    for k, v in mapping.items():\n",
        "        weighted_count = 0\n",
        "        for i in v:\n",
        "            weighted_count += weights[i]\n",
        "        weighted_counts[k] = weighted_count\n",
        "    weighted_counts_array = np.array(list(weighted_counts.values()))\n",
        "    p = np.true_divide(weighted_counts_array, weighted_counts_array.sum())\n",
        "    pe = -np.multiply(p, np.log2(p)).sum()\n",
        "    if normalize:\n",
        "        pe /= np.log2(factorial(order))\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ptX5ijEq7k0"
      },
      "outputs": [],
      "source": [
        "def hjorth_params(x, axis=-1):\n",
        "    x = np.asarray(x)\n",
        "    # Calculate derivatives\n",
        "    dx = np.diff(x, axis=axis)\n",
        "    ddx = np.diff(dx, axis=axis)\n",
        "    # Calculate variance\n",
        "    x_var = np.var(x, axis=axis)  # = activity\n",
        "    dx_var = np.var(dx, axis=axis)\n",
        "    ddx_var = np.var(ddx, axis=axis)\n",
        "    # Mobility and complexity\n",
        "    act = x_var\n",
        "    mob = np.sqrt(dx_var / x_var)\n",
        "    com = np.sqrt(ddx_var / dx_var) / mob\n",
        "    return act, mob, com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_3CcY_PNcj0"
      },
      "outputs": [],
      "source": [
        "def petrosian_fd(data, axis=-1):  # Petrosian fractal dimension\n",
        "    x = np.asarray(data)\n",
        "    N = x.shape[axis]\n",
        "    nzc_deriv = ent.num_zerocross(np.diff(x, axis=axis), axis=axis) # Number of sign changes in the first derivative of the signal\n",
        "    pfd = np.log10(N) / (np.log10(N) + np.log10(N / (N + 0.4 * nzc_deriv)))\n",
        "    return pfd\n",
        "def hjorth_parameter(data): # Hjorth mobility and complexity\n",
        "    hjorth_value = ent.hjorth_params(data)\n",
        "    return (hjorth_value[0],hjorth_value[1])\n",
        "def extrema(arr): # function to find local extremum\n",
        "    n = arr.shape[0]\n",
        "    count = 0\n",
        "    a = arr.tolist()\n",
        "    for i in range(1, n - 1) :  # start loop from position 1 till n-1\n",
        "        # only one condition will be true at a time either a[i] will be greater than neighbours or less than neighbours\n",
        "        # check if a[i] if greater than both its neighbours, then add 1 to x\n",
        "        count += (a[i]>a[i-1] and a[i]>a[i+1]);\n",
        "        # check if a[i] if less than both its neighbours, then add 1 to x\n",
        "        count += (a[i] < a[i - 1] and a[i] < a[i + 1]);\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xc4GHO7NzXE"
      },
      "outputs": [],
      "source": [
        "def energy_in_each_frq_band(psd,frqs): # PSD  -->   Frequency Bands  -->  energy\n",
        "    delta_low_freq, delta_high_freq = 0.5, 4\n",
        "    theta_low_freq, theta_high_freq = 4, 8\n",
        "    alpha_low_freq, alpha_high_freq = 8, 12\n",
        "    beta_low_freq , beta_high_freq  = 12, 30\n",
        "    gamma_low_freq , gamma_high_freq = 30, 48\n",
        "    idx_delta = np.logical_and(freqs >= delta_low_freq, freqs <= delta_high_freq)\n",
        "    idx_theta = np.logical_and(freqs >= theta_low_freq, freqs <= theta_high_freq)\n",
        "    idx_alpha = np.logical_and(freqs >= alpha_low_freq, freqs <= alpha_high_freq)\n",
        "    idx_beta  = np.logical_and(freqs >= beta_low_freq, freqs <= beta_high_freq)\n",
        "    idx_gamma = np.logical_and(freqs >= gamma_low_freq, freqs <= gamma_high_freq)\n",
        "    delta_energy = energy(psd[idx_delta])\n",
        "    theta_energy = energy(psd[idx_theta])\n",
        "    alpha_energy = energy(psd[idx_alpha])\n",
        "    beta_energy = energy(psd[idx_beta])\n",
        "    gamma_energy = energy(psd[idx_gamma])\n",
        "    return delta_energy, theta_energy, alpha_energy, beta_energy, gamma_energy\n",
        "def IWMF(psd,frqs):\n",
        "    iwmf, temp = 0, 0\n",
        "    for i in range(psd.shape[0]):\n",
        "        temp = psd[i]*frqs[i]\n",
        "        iwmf = iwmf + temp\n",
        "    return iwmf\n",
        "def IWBW(psd, frqs):\n",
        "    iwbw_1 = 0\n",
        "    iwmf = IWMF(psd, frqs)\n",
        "    for i in range(psd.shape[0]):\n",
        "        temp_1 = (frqs[i]-iwmf)**2\n",
        "        temp_2 = temp_1*psd[i]\n",
        "        iwbw_1 = temp_2 + iwbw_1\n",
        "    return math.sqrt(iwbw_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbIxt_6iOM1u"
      },
      "outputs": [],
      "source": [
        "def calcNormalizedFFT(epoch,lvl,nt,fs=128):\n",
        "    lseg = np.round(nt/fs*lvl).astype('int')\n",
        "    D = np.absolute(np.fft.fft(epoch, n=lseg[-1]))\n",
        "    D /= D.sum()\n",
        "    return D\n",
        "def SpectralEdgeFreq(epoch, lvl): # find the spectral edge frequency\n",
        "    nt, fs, percent, tfreq = 18, 512, 0.5, 40\n",
        "    sfreq, ppow= fs, percent\n",
        "    topfreq = int(round(nt/sfreq*tfreq)) + 1\n",
        "    D = calcNormalizedFFT(epoch, lvl, nt, fs)\n",
        "    A = np.cumsum(D[:topfreq])\n",
        "    B = A - (A.max()*ppow)\n",
        "    spedge = np.min(np.abs(B))\n",
        "    spedge = (spedge - 1)/(topfreq - 1)*tfreq\n",
        "    return spedge\n",
        "def DWT(x):\n",
        "    resp = pywt.dwt(x, 'db4') # single level decompositions\n",
        "    resp = np.ravel(np.array(resp[0])) \n",
        "    shyam = pywt.wavedec(resp, wavelet = 'db4', level=3) # it will return 4 labels of decompositions\n",
        "    return (shyam[0], shyam[1], shyam[2], shyam[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-ZDHZ-qOifs"
      },
      "outputs": [],
      "source": [
        "def sf_psd(psd):\n",
        "    mean_psd = np.mean(psd)\n",
        "    var_psd = np.var(psd)\n",
        "    mode_psd = float(st.mode(psd)[0])\n",
        "    median_psd = np.median(psd)\n",
        "    skew_psd = scipy.stats.skew(psd)\n",
        "    std_psd = np.std(psd)\n",
        "    kurtosis_psd = kurtosis(psd)\n",
        "    f_d_psd = first_difference(psd) \n",
        "    nfd_psd = f_d_psd/std_psd\n",
        "    s_d_psd = second_difference(psd)\n",
        "    nsd_psd = s_d_psd/std_psd\n",
        "    return mean_psd, var_psd, mode_psd, median_psd, skew_psd, std_psd, kurtosis_psd, f_d_psd, nfd_psd, s_d_psd, nsd_psd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kz87Z7iOsFs"
      },
      "outputs": [],
      "source": [
        "def sf_dwt(dwt):\n",
        "    mean_dwt = np.mean(dwt)\n",
        "    var_dwt = np.var(dwt)\n",
        "    mode_dwt = float(st.mode(dwt)[0])\n",
        "    median_dwt = np.median(dwt)\n",
        "    skew_dwt = scipy.stats.skew(dwt)\n",
        "    std_dwt = np.std(dwt)\n",
        "    kurtosis_dwt = kurtosis(dwt)\n",
        "    f_d_dwt = first_difference(dwt)\n",
        "    nfd_dwt = f_d_dwt/std_dwt\n",
        "    s_d_dwt = second_difference(dwt)\n",
        "    nsd_dwt = s_d_dwt/std_dwt\n",
        "    return mean_dwt, var_dwt, mode_dwt, median_dwt, skew_dwt, std_dwt, kurtosis_dwt, f_d_dwt, nfd_dwt, s_d_dwt, nsd_dwt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8-qCDTAPNKb"
      },
      "outputs": [],
      "source": [
        "def get_features(data, data_dwt, channel_no):\n",
        "  feature_vector = []\n",
        "  no_of_features = 257\n",
        "  feature = np.ones((40 , len(channel_no)*no_of_features + 10))\n",
        "  delta_left, theta_left, alpha_left, beta_left, gamma_left, delta_right, theta_right, alpha_right, beta_right, gamma_right = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
        "  for video in range(0, 40):\n",
        "    channel_count = 0\n",
        "    for channel in channel_no:\n",
        "      # -----------------------------------------------           Time-Domain Analysis      --------------------------------------------\n",
        "      # 1.-------------------------------  Statistical parameters-------------------------------------------\n",
        "      mean_result = np.mean(data[video,channel])\n",
        "      var_result = np.var(data[video,channel])\n",
        "      mode_result = float(st.mode(data[video,channel])[0])\n",
        "      median_result = np.median(data[video,channel])\n",
        "      skew_result = scipy.stats.skew(data[video,channel])\n",
        "      std_result = np.std(data[video,channel])\n",
        "      kurtosis_result = kurtosis(data[video,channel])\n",
        "      f_d = first_difference(data[video,channel]) \n",
        "      normalised_first_difference = f_d/std_result\n",
        "      s_d = second_difference(data[video,channel])\n",
        "      normalised_second_difference = s_d/std_result\n",
        "\n",
        "      # 2. Energy, Average Power, Root mean square value(RMS) \n",
        "      energy_value = energy(data[video,channel])\n",
        "      avg_value = average_power(data[video,channel])\n",
        "      rms_result = np.sqrt(np.mean(data[video,channel]**2))\n",
        "      \n",
        "      # 3. Katz fractal dimension (line length// curve length//total vertical length)\n",
        "      katz_val = katz_fractal_dimension(data[video,channel])\n",
        "        \n",
        "      # 4. Nonlinear energy(NE)\n",
        "      nle_val = non_linear_energy(data[video,channel])\n",
        "      \n",
        "      # 5. Shannon entropy (shEn)\n",
        "      ShEn_result = shannon_entopy(data[video,channel])\n",
        "      \n",
        "      # 6. Approximate entropy\n",
        "      entropy_value = ent.app_entropy(data[video,channel])\n",
        "        \n",
        "      # 7. Sample entropy\n",
        "      sample_entropy = ent.sample_entropy(data[video,channel])\n",
        "        \n",
        "      # 8. Permutation entropy\n",
        "      perm_entropy_val = ent.perm_entropy(data[video,channel], normalize=True)\n",
        "        \n",
        "      # 9. Weigheted Permutation Entropy \n",
        "      WPE = weighted_permutation_entropy(data[video,channel], order=3, normalize=False)\n",
        "\n",
        "      # 10. Singular Value Decomposition \n",
        "      svd_entropy_val = ent.svd_entropy(data[video,channel], normalize=True) # Singular value decomposition entropy\n",
        "        \n",
        "      # 11. Hurst Exponent(HE)\n",
        "      # Here we have two paramaters of HE i.e. H and c\n",
        "      H, c, data_HC = compute_Hc(data[video,channel], kind='change', simplified=True)\n",
        "        \n",
        "      # 12. Fractal dimention\n",
        "      higuchi_val = ent.higuchi_fd(data[video,channel])   # Higuchi fractal dimension\n",
        "      petrosian_val = petrosian_fd(data[video,channel])   # Petrosian fractal dimension\n",
        "        \n",
        "      # 13. Hjorth mobility and complexity\n",
        "      hjorth_avability, hjorth_mobilty, hjorth_complexity = hjorth_params(data[video,channel])   # Hjorth mobility and complexity\n",
        "        \n",
        "      # 14. Detrended Fluctuation Analysis (DFA)\n",
        "      DFA = ent.detrended_fluctuation(data[video,channel])\n",
        "        \n",
        "      # 15. Number of zero-crossings\n",
        "      num_zerocross_val = ent.num_zerocross(data[video,channel])   # Number of zero-crossings\n",
        "        \n",
        "      # 16. Number of local extraimum\n",
        "      local_extrema = extrema(data[video,channel])\n",
        "        \n",
        "      #--------------------------------------------------      Frequency Domain Analysis    ----------------------------\n",
        "      # Power Spectral Density (PSD)\n",
        "      psd,freqs = plt.psd(data[video,channel], Fs = 128)\n",
        "      delta_low_freq, delta_high_freq = 0.5, 4\n",
        "      theta_low_freq, theta_high_freq = 4, 8\n",
        "      alpha_low_freq, alpha_high_freq = 8, 12\n",
        "      beta_low_freq , beta_high_freq  = 12, 30\n",
        "      gamma_low_freq , gamma_high_freq = 30, 48\n",
        "      idx_delta = np.logical_and(freqs >= delta_low_freq, freqs <= delta_high_freq)\n",
        "      idx_theta = np.logical_and(freqs >= theta_low_freq, freqs <= theta_high_freq)\n",
        "      idx_alpha = np.logical_and(freqs >= alpha_low_freq, freqs <= alpha_high_freq)\n",
        "      idx_beta  = np.logical_and(freqs >= beta_low_freq, freqs <= beta_high_freq)\n",
        "      idx_gamma = np.logical_and(freqs >= gamma_low_freq, freqs <= gamma_high_freq)\n",
        "        \n",
        "      # Normalised psd\n",
        "      # normalised_psd  = psd/energy(psd)\n",
        "      \n",
        "      # -------------------------   2. Energy, Average Power, Root mean square value(RMS) ---------------------\n",
        "      # Energy calculation for each band\n",
        "      delta_energy = energy(psd[idx_delta])\n",
        "      theta_energy = energy(psd[idx_theta])\n",
        "      alpha_energy = energy(psd[idx_alpha])\n",
        "      beta_energy  = energy(psd[idx_beta])\n",
        "      gamma_energy = energy(psd[idx_gamma])\n",
        "        \n",
        "      # Average power for each band\n",
        "      delta_avg_power = average_power(psd[idx_delta])\n",
        "      theta_avg_power = average_power(psd[idx_theta])\n",
        "      alpha_avg_power = average_power(psd[idx_alpha])\n",
        "      beta_avg_power  = average_power(psd[idx_beta])\n",
        "      gamma_avg_power = average_power(psd[idx_gamma])\n",
        "        \n",
        "      # RMS value for each band\n",
        "      delta_rms = np.sqrt(np.mean(psd[idx_delta]**2))\n",
        "      theta_rms = np.sqrt(np.mean(psd[idx_theta]**2))\n",
        "      alpha_rms = np.sqrt(np.mean(psd[idx_alpha]**2))\n",
        "      beta_rms  = np.sqrt(np.mean(psd[idx_beta]**2))\n",
        "      gamma_rms = np.sqrt(np.mean(psd[idx_gamma]**2))\n",
        "\n",
        "      # 2. Intensity weighted mean frequency (IWMF)\n",
        "      iwmf = IWMF(psd,freqs)\n",
        "        \n",
        "      # 3. Intensity weighted bandwidth (IWBW)\n",
        "      iwbw = IWBW(psd,freqs)\n",
        "        \n",
        "      # 4. Spectral Edge Frequency applied after apllying Discrete Fourier Transform on the raw data\n",
        "      sef = SpectralEdgeFreq(psd,freqs)\n",
        "        \n",
        "      # 5. Spectral Entropy\n",
        "      spectral_entropy_val = ent.spectral_entropy(data[video,channel], sf=128, method='welch', normalize=True)  # Spectral entropy\n",
        "        \n",
        "      # 6. Peak Frequencies\n",
        "      peaks, _ = find_peaks(psd, height = 0)\n",
        "      peak_values = psd[peaks]\n",
        "      avg_peak_value = np.mean(psd[peaks]) # main feature column\n",
        "        \n",
        "      # 7. Rational Asymmetry (RASM) and Differential Asymmetry (DASM) features\n",
        "      # will be added at the last column of the feature vector\n",
        "      if(channel < 16): # left\n",
        "        delta_left =  delta_left + delta_avg_power\n",
        "        theta_left =  theta_left + theta_avg_power\n",
        "        alpha_left =  alpha_left + alpha_avg_power\n",
        "        beta_left  =  beta_left  + beta_avg_power \n",
        "        gamma_left =  gamma_left + gamma_avg_power\n",
        "            \n",
        "      if(channel >=16):  # right\n",
        "        delta_right = delta_right + delta_avg_power\n",
        "        theta_right = theta_right + theta_avg_power\n",
        "        alpha_right = alpha_right + alpha_avg_power\n",
        "        beta_right  = beta_right  + beta_avg_power \n",
        "        gamma_right = gamma_right + gamma_avg_power\n",
        "      # 8. statistical features are computed from the psd feature extraction\n",
        "      # PSD calculation\n",
        "      delta_mean_psd, delta_var_psd, delta_mode_psd, delta_median_psd, delta_skew_psd, delta_std_psd, delta_kurtosis_psd, delta_f_d_psd, delta_nfd_psd, delta_s_d_psd, delta_nsd_psd = sf_psd(psd[idx_delta])\n",
        "      theta_mean_psd, theta_var_psd, theta_mode_psd, theta_median_psd, theta_skew_psd, theta_std_psd, theta_kurtosis_psd, theta_f_d_psd, theta_nfd_psd, theta_s_d_psd, theta_nsd_psd = sf_psd(psd[idx_theta])\n",
        "      alpha_mean_psd, alpha_var_psd, alpha_mode_psd, alpha_median_psd, alpha_skew_psd, alpha_std_psd, alpha_kurtosis_psd, alpha_f_d_psd, alpha_nfd_psd, alpha_s_d_psd, alpha_nsd_psd = sf_psd(psd[idx_alpha]) \n",
        "      beta_mean_psd , beta_var_psd,  beta_mode_psd,  beta_median_psd,  beta_skew_psd,  beta_std_psd,  beta_kurtosis_psd,  beta_f_d_psd,  beta_nfd_psd,  beta_s_d_psd,  beta_nsd_psd  = sf_psd(psd[idx_beta]) \n",
        "      gamma_mean_psd, gamma_var_psd, gamma_mode_psd, gamma_median_psd, gamma_skew_psd, gamma_std_psd, gamma_kurtosis_psd, gamma_f_d_psd, gamma_nfd_psd, gamma_s_d_psd, gamma_nsd_psd = sf_psd(psd[idx_gamma])\n",
        "      #---------------------------------------------------     Wavelet feature extraction    ------------------------------------\n",
        "      \"\"\"\n",
        "      Discreate Wavelet Feature\n",
        "      dwt_single = pywt.dwt(data[video,channel], 'db4') # single level decompositions\n",
        "      print(len(dwt_single))\n",
        "      CA, CD = np.ravel(np.array(dwt_single[0])), np.ravel(np.array(dwt_single[1]))\n",
        "      print(CA, CD)\n",
        "      \"\"\"\n",
        "      coeffs = wavedec(data_dwt[video,channel], 'db1', level=4)\n",
        "      delta, theta, alpha, beta, gamma = coeffs\n",
        "      #-----------------------------------------------------------------------------------------------------------------------\n",
        "        \n",
        "      # 1. Statistical feature are computed from the DWT feature which is decomposed over 0-64 Hz data\n",
        "      delta_mean_dwt, delta_var_dwt, delta_mode_dwt, delta_median_dwt, delta_skew_dwt, delta_std_dwt, delta_kurtosis_dwt, delta_f_d_dwt, delta_nfd_dwt, delta_s_d_dwt, delta_nsd_dwt = sf_psd(delta)\n",
        "      theta_mean_dwt, theta_var_dwt, theta_mode_dwt, theta_median_dwt, theta_skew_dwt, theta_std_dwt, theta_kurtosis_dwt, theta_f_d_dwt, theta_nfd_dwt, theta_s_d_dwt, theta_nsd_dwt = sf_psd(theta)\n",
        "      alpha_mean_dwt, alpha_var_dwt, alpha_mode_dwt, alpha_median_dwt, alpha_skew_dwt, alpha_std_dwt, alpha_kurtosis_dwt, alpha_f_d_dwt, alpha_nfd_dwt, alpha_s_d_dwt, alpha_nsd_dwt = sf_psd(alpha) \n",
        "      beta_mean_dwt , beta_var_dwt,  beta_mode_dwt,  beta_median_dwt,  beta_skew_dwt,  beta_std_dwt,  beta_kurtosis_dwt,  beta_f_d_dwt,  beta_nfd_dwt,  beta_s_d_dwt,  beta_nsd_dwt  = sf_psd(beta)\n",
        "      gamma_mean_dwt, gamma_var_dwt, gamma_mode_dwt, gamma_median_dwt, gamma_skew_dwt, gamma_std_dwt, gamma_kurtosis_dwt, gamma_f_d_dwt, gamma_nfd_dwt, gamma_s_d_dwt, gamma_nsd_dwt = sf_psd(gamma)\n",
        "      #----------------------------------------------------------------------------------------------------------------------\n",
        "        \n",
        "      # 2. Energy, Average Power  and RMS\n",
        "      # 2.1. Energy calculation for each band\n",
        "      delta_dwt_energy, theta_dwt_energy, alpha_dwt_energy, beta_dwt_energy, gamma_dwt_energy = energy(delta), energy(theta), energy(alpha), energy(beta), energy(gamma)\n",
        "\n",
        "      # 2.2. Average power for each band\n",
        "      delta_dwt_avg_power, theta_dwt_avg_power, alpha_dwt_avg_power, beta_dwt_avg_power, gamma_dwt_avg_power = average_power(delta), average_power(theta), average_power(alpha), average_power(beta), average_power(gamma)\n",
        "\n",
        "      # 2.3. RMS value for each band\n",
        "      delta_dwt_rms, theta_dwt_rms, alpha_dwt_rms, beta_dwt_rms, gamma_dwt_rms = np.sqrt(np.mean(delta**2)), np.sqrt(np.mean(theta**2)), np.sqrt(np.mean(alpha**2)), np.sqrt(np.mean(beta**2)), np.sqrt(np.mean(gamma**2))\n",
        "        \n",
        "      # 3. shEn, ApEn , PE, WPE\n",
        "      # 3.1 Shannon entropy (shEn)\n",
        "      delta_dwt_ShEn, theta_dwt_ShEn, alpha_dwt_ShEn, beta_dwt_ShEn, gamma_dwt_ShEn = shannon_entopy(delta), shannon_entopy(theta), shannon_entopy(alpha), shannon_entopy(beta), shannon_entopy(gamma)\n",
        "        \n",
        "      # 3.2 Approximate entropy\n",
        "      delta_dwt_aentropy, theta_dwt_aentropy, alpha_dwt_aentropy, beta_dwt_aentropy, gamma_dwt_aentropy = ent.app_entropy(delta), ent.app_entropy(theta), ent.app_entropy(alpha), ent.app_entropy(beta), ent.app_entropy(gamma)\n",
        "        \n",
        "      # 3.3 Permutation entropy\n",
        "      delta_dwt_pentropy, theta_dwt_pentropy, alpha_dwt_pentropy, beta_dwt_pentropy, gamma_dwt_pentropy = ent.perm_entropy(delta, normalize=True), ent.perm_entropy(theta, normalize=True), ent.perm_entropy(alpha, normalize=True), ent.perm_entropy(beta, normalize=True), ent.perm_entropy(gamma, normalize=True)\n",
        "        \n",
        "      # 3.4 Weigheted Permutation Entropy \n",
        "      delta_dwt_wpe, theta_dwt_wpe, alpha_dwt_wpe = weighted_permutation_entropy(delta, order=3, normalize=False), weighted_permutation_entropy(theta, order=3, normalize=False), weighted_permutation_entropy(alpha, order=3, normalize=False)\n",
        "      beta_dwt_wpe, gamma_dwt_wpe = weighted_permutation_entropy(beta, order=3, normalize=False), weighted_permutation_entropy(gamma, order=3, normalize=False)\n",
        "\n",
        "      # 4. Hurst Exponent(HE)\n",
        "      # Here we have two paramaters of HE i.e. H and c\n",
        "      H_delta, c_delta, data_HC_delta = compute_Hc(delta, kind='change', simplified=True)\n",
        "      H_theta, c_theta, data_HC_theta = compute_Hc(theta, kind='change', simplified=True)\n",
        "      H_alpha, c_alpha, data_HC_alpha = compute_Hc(alpha, kind='change', simplified=True)\n",
        "      H_beta, c_beta, data_HC_beta = compute_Hc(beta, kind='change', simplified=True)\n",
        "      H_gamma, c_gamma, data_HC_gamma = compute_Hc(gamma, kind='change', simplified=True)\n",
        "        \n",
        "      # 5. Fractal dimention\n",
        "      higuchi_delta   = ent.higuchi_fd(delta)   # Higuchi fractal dimension for delta\n",
        "      petrosian_delta = petrosian_fd(delta)   # Petrosian fractal dimension for delta\n",
        "      higuchi_theta   = ent.higuchi_fd(theta)   # Higuchi fractal dimension for theta\n",
        "      petrosian_theta = petrosian_fd(theta)   # Petrosian fractal dimension for theta\n",
        "      higuchi_alpha   = ent.higuchi_fd(alpha)   # Higuchi fractal dimension for alpha\n",
        "      petrosian_alpha = petrosian_fd(alpha)   # Petrosian fractal dimension for alpha\n",
        "      higuchi_beta   = ent.higuchi_fd(beta)   # Higuchi fractal dimension for beta\n",
        "      petrosian_beta = petrosian_fd(beta)   # Petrosian fractal dimension for beta\n",
        "      higuchi_gamma   = ent.higuchi_fd(gamma)   # Higuchi fractal dimension for gamma\n",
        "      petrosian_gamma = petrosian_fd(gamma)   # Petrosian fractal dimension for gamma\n",
        "        \n",
        "      # 6. Auto regressive (AR)\n",
        "      res_delta = AutoReg(delta,lags = 128).fit()\n",
        "      res_theta = AutoReg(theta,lags = 128).fit()\n",
        "      res_alpha = AutoReg(alpha,lags = 128).fit()\n",
        "      res_beta = AutoReg(beta,lags = 128).fit()\n",
        "      res_gamma = AutoReg(gamma,lags = 128).fit()\n",
        "      aic_delta_ar, hqic_delta_ar, bic_delta_ar, llf_delta_ar = res_delta.aic, res_delta.hqic, res_delta.bic, res_delta.llf\n",
        "      aic_theta_ar, hqic_theta_ar, bic_theta_ar, llf_theta_ar = res_theta.aic, res_theta.hqic, res_theta.bic, res_theta.llf\n",
        "      aic_alpha_ar, hqic_alpha_ar, bic_alpha_ar, llf_alpha_ar = res_alpha.aic, res_alpha.hqic, res_alpha.bic, res_alpha.llf\n",
        "      aic_beta_ar, hqic_beta_ar, bic_beta_ar, llf_beta_ar = res_beta.aic, res_beta.hqic, res_beta.bic, res_beta.llf\n",
        "      aic_gamma_ar, hqic_gamma_ar, bic_gamma_ar, llf_gamma_ar = res_gamma.aic, res_gamma.hqic, res_gamma.bic, res_gamma.llf\n",
        "\n",
        "\n",
        "      # 7. Autoregressive moving Average (ARMA)\n",
        "      try:\n",
        "        arma_delta = stats.ARIMA(delta, order=(5,1,0)).fit()\n",
        "      except:\n",
        "        arma_delta = stats.ARIMA(delta, order=(3,1,0)).fit()\n",
        "      try:\n",
        "        arma_theta = stats.ARIMA(theta, order=(5,1,0)).fit()\n",
        "      except:\n",
        "        arma_theta = stats.ARIMA(theta, order=(3,1,0)).fit()\n",
        "      try:\n",
        "        arma_alpha = stats.ARIMA(alpha, order=(5,1,0)).fit()\n",
        "      except:\n",
        "        arma_alpha = stats.ARIMA(alpha, order=(3,1,0)).fit()\n",
        "      try:\n",
        "        arma_beta = stats.ARIMA(beta, order=(5,1,0)).fit()\n",
        "      except:\n",
        "        arma_beta = stats.ARIMA(beta, order=(3,1,0)).fit()\n",
        "      try:\n",
        "        arma_gamma = stats.ARIMA(gamma, order=(5,1,0)).fit()\n",
        "      except:\n",
        "        arma_gamma = stats.ARIMA(gamma, order=(3,1,0)).fit()\n",
        "\n",
        "      aic_delta_arma, hqic_delta_arma, bic_delta_arma, llf_delta_arma = arma_delta.aic, arma_delta.hqic, arma_delta.bic, arma_delta.llf\n",
        "      aic_theta_arma, hqic_theta_arma, bic_theta_arma, llf_theta_arma = arma_theta.aic, arma_theta.hqic, arma_theta.bic, arma_theta.llf\n",
        "      aic_alpha_arma, hqic_alpha_arma, bic_alpha_arma, llf_alpha_arma = arma_alpha.aic, arma_alpha.hqic, arma_alpha.bic, arma_alpha.llf\n",
        "      aic_beta_arma, hqic_beta_arma, bic_beta_arma, llf_beta_arma = arma_beta.aic, arma_beta.hqic, arma_beta.bic, arma_beta.llf\n",
        "      aic_gamma_arma, hqic_gamma_arma, bic_gamma_arma, llf_gamma_arma = arma_gamma.aic, arma_gamma.hqic, arma_gamma.bic, arma_gamma.llf\n",
        "      #--------------------------------------------------------------------------------------------------------------------------\n",
        "      feature_vector = [mean_result, var_result, mode_result, median_result, skew_result, std_result, kurtosis_result, f_d,\n",
        "                        normalised_first_difference, s_d, normalised_second_difference, energy_value, avg_value, rms_result,\n",
        "                        katz_val, nle_val, ShEn_result, entropy_value, sample_entropy, perm_entropy_val, WPE, svd_entropy_val, H, c, \n",
        "                        higuchi_val, petrosian_val, hjorth_avability, hjorth_mobilty, hjorth_complexity, DFA, num_zerocross_val, local_extrema, \n",
        "                        delta_energy, theta_energy, alpha_energy, beta_energy, gamma_energy, delta_avg_power, theta_avg_power, alpha_avg_power, \n",
        "                        beta_avg_power, gamma_avg_power, delta_rms, theta_rms, alpha_rms, beta_rms, gamma_rms, iwmf, iwbw, sef, spectral_entropy_val, \n",
        "                        avg_peak_value, delta_mean_psd, delta_var_psd, delta_mode_psd, delta_median_psd, delta_skew_psd, delta_std_psd, \n",
        "                        delta_kurtosis_psd, delta_f_d_psd, delta_nfd_psd, delta_s_d_psd, delta_nsd_psd , theta_mean_psd, theta_var_psd, theta_mode_psd, \n",
        "                        theta_median_psd, theta_skew_psd, theta_std_psd, theta_kurtosis_psd, theta_f_d_psd, theta_nfd_psd, theta_s_d_psd, theta_nsd_psd,\n",
        "                        alpha_mean_psd, alpha_var_psd, alpha_mode_psd, alpha_median_psd, alpha_skew_psd, alpha_std_psd, alpha_kurtosis_psd,\n",
        "                        alpha_f_d_psd, alpha_nfd_psd, alpha_s_d_psd, alpha_nsd_psd, beta_mean_psd, beta_var_psd,  beta_mode_psd,  \n",
        "                        beta_median_psd,  beta_skew_psd,  beta_std_psd,  beta_kurtosis_psd,  beta_f_d_psd,  beta_nfd_psd,  beta_s_d_psd,\n",
        "                        beta_nsd_psd, gamma_mean_psd, gamma_var_psd, gamma_mode_psd, gamma_median_psd, gamma_skew_psd, gamma_std_psd, \n",
        "                        gamma_kurtosis_psd, gamma_f_d_psd, gamma_nfd_psd, gamma_s_d_psd, gamma_nsd_psd,\n",
        "                        delta_mean_dwt, delta_var_dwt, delta_mode_dwt, delta_median_dwt, delta_skew_dwt, delta_std_dwt, \n",
        "                        delta_kurtosis_dwt, delta_f_d_dwt, delta_nfd_dwt, delta_s_d_dwt, delta_nsd_dwt, theta_mean_dwt, theta_var_dwt, theta_mode_dwt, \n",
        "                        theta_median_dwt, theta_skew_dwt, theta_std_dwt, theta_kurtosis_dwt, theta_f_d_dwt, theta_nfd_dwt, theta_s_d_dwt, theta_nsd_dwt,\n",
        "                        alpha_mean_dwt, alpha_var_dwt, alpha_mode_dwt, alpha_median_dwt, alpha_skew_dwt, alpha_std_dwt, alpha_kurtosis_dwt,\n",
        "                        alpha_f_d_dwt, alpha_nfd_dwt, alpha_s_d_dwt, alpha_nsd_dwt, beta_mean_dwt, beta_var_dwt, beta_mode_dwt,  \n",
        "                        beta_median_dwt, beta_skew_dwt, beta_std_dwt, beta_kurtosis_dwt, beta_f_d_dwt, beta_nfd_dwt,  beta_s_d_dwt,\n",
        "                        beta_nsd_dwt, gamma_mean_dwt, gamma_var_dwt, gamma_mode_dwt, gamma_median_dwt, gamma_skew_dwt, gamma_std_dwt, \n",
        "                        gamma_kurtosis_dwt, gamma_f_d_dwt, gamma_nfd_dwt, gamma_s_d_dwt, gamma_nsd_dwt, delta_dwt_energy, theta_dwt_energy,\n",
        "                        alpha_dwt_energy, beta_dwt_energy, gamma_dwt_energy, delta_dwt_avg_power, theta_dwt_avg_power, alpha_dwt_avg_power, \n",
        "                        beta_dwt_avg_power, gamma_dwt_avg_power, delta_dwt_rms, theta_dwt_rms, alpha_dwt_rms, beta_dwt_rms, gamma_dwt_rms,\n",
        "                        delta_dwt_ShEn, theta_dwt_ShEn, alpha_dwt_ShEn, beta_dwt_ShEn, gamma_dwt_ShEn, delta_dwt_aentropy, theta_dwt_aentropy,\n",
        "                        alpha_dwt_aentropy, beta_dwt_aentropy, gamma_dwt_aentropy, delta_dwt_pentropy, theta_dwt_pentropy, alpha_dwt_pentropy,\n",
        "                        beta_dwt_pentropy, gamma_dwt_pentropy, delta_dwt_wpe, theta_dwt_wpe, alpha_dwt_wpe, beta_dwt_wpe, gamma_dwt_wpe,\n",
        "                        H_delta, c_delta, H_theta, c_theta, H_alpha, c_alpha, H_beta, c_beta, H_gamma, c_gamma, higuchi_delta, petrosian_delta, \n",
        "                        higuchi_theta, petrosian_theta, higuchi_alpha, petrosian_alpha, higuchi_beta, petrosian_beta, higuchi_gamma, petrosian_gamma,\n",
        "                        aic_delta_ar, hqic_delta_ar, bic_delta_ar, llf_delta_ar, aic_theta_ar, hqic_theta_ar, bic_theta_ar, llf_theta_ar, aic_alpha_ar, \n",
        "                        hqic_alpha_ar, bic_alpha_ar, llf_alpha_ar, aic_beta_ar, hqic_beta_ar, bic_beta_ar, llf_beta_ar, aic_gamma_ar, hqic_gamma_ar, \n",
        "                        bic_gamma_ar, llf_gamma_ar, aic_delta_arma, hqic_delta_arma, bic_delta_arma, llf_delta_arma, aic_theta_arma, hqic_theta_arma, \n",
        "                        bic_theta_arma, llf_theta_arma, aic_alpha_arma, hqic_alpha_arma, bic_alpha_arma, llf_alpha_arma, aic_beta_arma, hqic_beta_arma, \n",
        "                        bic_beta_arma, llf_beta_arma, aic_gamma_arma, hqic_gamma_arma, bic_gamma_arma, llf_gamma_arma]\n",
        "      #feature_vector = np.concatenate((feature_vector,dwt_val))\n",
        "      #print(type(feature_vector))\n",
        "      #print(feature_vector[10])\n",
        "      for i in range(0, no_of_features):\n",
        "        #print(feature_vector[i],i)\n",
        "        feature[video][channel_count + i] = feature_vector[i]\n",
        "      channel_count = channel_count + no_of_features\n",
        "    \n",
        "    delta_rasm = delta_left/delta_right\n",
        "    theta_rasm = theta_left/theta_right\n",
        "    alpha_rasm = alpha_left/alpha_right\n",
        "    beta_rasm  = beta_left/beta_right\n",
        "    gamma_rasm = gamma_left/gamma_right\n",
        "    \n",
        "    delta_dasm = delta_left - delta_right\n",
        "    theta_dasm = theta_left - theta_right\n",
        "    alpha_dasm = alpha_left - alpha_right\n",
        "    beta_dasm  = beta_left  - beta_right\n",
        "    gamma_dasm = gamma_left - gamma_right\n",
        "    fv = [delta_rasm, theta_rasm, alpha_rasm, beta_rasm, gamma_rasm, delta_dasm, theta_dasm, alpha_dasm, beta_dasm, gamma_dasm]\n",
        "    c_draft = 0\n",
        "    for i in range(feature.shape[1] - len(fv), feature.shape[1]):\n",
        "      feature[video][i] = fv[c_draft]\n",
        "      c_draft = c_draft + 1\n",
        "    print(\"video: \", video)\n",
        "  return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaNh4H5f3g23"
      },
      "outputs": [],
      "source": [
        "def all_channels_name():\n",
        "  channels = ['mean_result', 'var_result', 'mode_result', 'median_result', 'skew_result', 'std_result', 'kurtosis_result', 'f_d',\n",
        "              'normalised_first_difference', 's_d', 'normalised_second_difference', 'energy_value', 'avg_value', 'rms_result',\n",
        "              'katz_val', 'nle_val', 'ShEn_result', 'entropy_value', 'sample_entropy', 'perm_entropy_val', 'WPE', 'svd_entropy_val', 'H', 'c', \n",
        "              'higuchi_val', 'petrosian_val', 'hjorth_avability', 'hjorth_mobilty', 'hjorth_complexity', 'DFA', 'num_zerocross_val', 'local_extrema',\n",
        "              'delta_energy', 'theta_energy', 'alpha_energy', 'beta_energy', 'gamma_energy', 'delta_avg_power', 'theta_avg_power', 'alpha_avg_power',\n",
        "              'beta_avg_power', 'gamma_avg_power', 'delta_rms', 'theta_rms', 'alpha_rms', 'beta_rms', 'gamma_rms', 'iwmf', 'iwbw', 'sef', 'spectral_entropy_val', \n",
        "              'avg_peak_value', 'delta_mean_psd', 'delta_var_psd', 'delta_mode_psd', 'delta_median_psd', 'delta_skew_psd', 'delta_std_psd', \n",
        "              'delta_kurtosis_psd', 'delta_f_d_psd', 'delta_nfd_psd', 'delta_s_d_psd', 'delta_nsd_psd', 'theta_mean_psd', 'theta_var_psd', 'theta_mode_psd', \n",
        "              'theta_median_psd', 'theta_skew_psd', 'theta_std_psd', 'theta_kurtosis_psd', 'theta_f_d_psd', 'theta_nfd_psd', 'theta_s_d_psd', 'theta_nsd_psd',\n",
        "              'alpha_mean_psd', 'alpha_var_psd', 'alpha_mode_psd', 'alpha_median_psd', 'alpha_skew_psd', 'alpha_std_psd', 'alpha_kurtosis_psd',\n",
        "              'alpha_f_d_psd', 'alpha_nfd_psd', 'alpha_s_d_psd', 'alpha_nsd_psd', 'beta_mean_psd', 'beta_var_psd', 'beta_mode_psd',  \n",
        "              'beta_median_psd', 'beta_skew_psd', 'beta_std_psd', 'beta_kurtosis_psd', 'beta_f_d_psd', 'beta_nfd_psd', 'beta_s_d_psd',\n",
        "              'beta_nsd_psd', 'gamma_mean_psd', 'gamma_var_psd', 'gamma_mode_psd', 'gamma_median_psd', 'gamma_skew_psd', 'gamma_std_psd', \n",
        "              'gamma_kurtosis_psd', 'gamma_f_d_psd', 'gamma_nfd_psd', 'gamma_s_d_psd', 'gamma_nsd_psd',\n",
        "              'delta_mean_dwt', 'delta_var_dwt', 'delta_mode_dwt', 'delta_median_dwt', 'delta_skew_dwt', 'delta_std_dwt', \n",
        "              'delta_kurtosis_dwt', 'delta_f_d_dwt', 'delta_nfd_dwt', 'delta_s_d_dwt', 'delta_nsd_dwt', 'theta_mean_dwt', 'theta_var_dwt', 'theta_mode_dwt', \n",
        "              'theta_median_dwt', 'theta_skew_dwt', 'theta_std_dwt', 'theta_kurtosis_dwt', 'theta_f_d_dwt', 'theta_nfd_dwt', 'theta_s_d_dwt', 'theta_nsd_dwt',\n",
        "              'alpha_mean_dwt', 'alpha_var_dwt', 'alpha_mode_dwt', 'alpha_median_dwt', 'alpha_skew_dwt', 'alpha_std_dwt', 'alpha_kurtosis_dwt',\n",
        "              'alpha_f_d_dwt', 'alpha_nfd_dwt', 'alpha_s_d_dwt', 'alpha_nsd_dwt', 'beta_mean_dwt', 'beta_var_dwt', 'beta_mode_dwt',  \n",
        "              'beta_median_dwt', 'beta_skew_dwt', 'beta_std_dwt', 'beta_kurtosis_dwt', 'beta_f_d_dwt', 'beta_nfd_dwt',  'beta_s_d_dwt',\n",
        "              'beta_nsd_dwt', 'gamma_mean_dwt', 'gamma_var_dwt', 'gamma_mode_dwt', 'gamma_median_dwt', 'gamma_skew_dwt', 'gamma_std_dwt', \n",
        "              'gamma_kurtosis_dwt', 'gamma_f_d_dwt', 'gamma_nfd_dwt', 'gamma_s_d_dwt', 'gamma_nsd_dwt',\n",
        "              'delta_dwt_energy', 'theta_dwt_energy', 'alpha_dwt_energy', 'beta_dwt_energy', 'gamma_dwt_energy', \n",
        "              'delta_dwt_avg_power', 'theta_dwt_avg_power', 'alpha_dwt_avg_power', 'beta_dwt_avg_power', 'gamma_dwt_avg_power', \n",
        "              'delta_dwt_rms', 'theta_dwt_rms', 'alpha_dwt_rms', 'beta_dwt_rms', 'gamma_dwt_rms',\n",
        "              'delta_dwt_ShEn', 'theta_dwt_ShEn', 'alpha_dwt_ShEn', 'beta_dwt_ShEn', 'gamma_dwt_ShEn', \n",
        "              'delta_dwt_aentropy', 'theta_dwt_aentropy', 'alpha_dwt_aentropy', 'beta_dwt_aentropy', 'gamma_dwt_aentropy', \n",
        "              'delta_dwt_pentropy', 'theta_dwt_pentropy', 'alpha_dwt_pentropy', 'beta_dwt_pentropy', 'gamma_dwt_pentropy', \n",
        "              'delta_dwt_wpe', 'theta_dwt_wpe', 'alpha_dwt_wpe', 'beta_dwt_wpe', 'gamma_dwt_wpe',\n",
        "              'H_delta', 'c_delta', 'H_theta', 'c_theta', 'H_alpha', 'c_alpha', 'H_beta', 'c_beta', 'H_gamma', 'c_gamma', \n",
        "              'higuchi_delta', 'petrosian_delta', 'higuchi_theta', 'petrosian_theta', 'higuchi_alpha', 'petrosian_alpha', 'higuchi_beta', 'petrosian_beta', 'higuchi_gamma', 'petrosian_gamma',\n",
        "              'aic_delta_ar', 'hqic_delta_ar', 'bic_delta_ar', 'llf_delta_ar',  'aic_theta_ar', 'hqic_theta_ar', 'bic_theta_ar', 'llf_theta_ar', 'aic_alpha_ar', \n",
        "              'hqic_alpha_ar', 'bic_alpha_ar', 'llf_alpha_ar', 'aic_beta_ar', 'hqic_beta_ar', 'bic_beta_ar', 'llf_beta_ar', 'aic_gamma_ar', 'hqic_gamma_ar', 'bic_gamma_ar', 'llf_gamma_ar',\n",
        "              'aic_delta_arma', 'hqic_delta_arma', 'bic_delta_arma', 'llf_delta_arma', 'aic_theta_arma', 'hqic_theta_arma', 'bic_theta_arma', 'llf_theta_arma',\n",
        "              'aic_alpha_arma', 'hqic_alpha_arma', 'bic_alpha_arma', 'llf_alpha_arma', 'aic_beta_arma', 'hqic_beta_arma', 'bic_beta_arma', 'llf_beta_arma',\n",
        "              'aic_gamma_arma', 'hqic_gamma_arma', 'bic_gamma_arma', 'llf_gamma_arma']\n",
        "  print(len(channels))\n",
        "  return channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDBnwhgB8ubW"
      },
      "outputs": [],
      "source": [
        "def get_channels_name(channel_no):\n",
        "  channels = all_channels_name()\n",
        "  channels_name = []\n",
        "  for i in eeg_channels[channel_no]:\n",
        "    for j in channels:\n",
        "      channels_name.append(i + \"_\" + j)\n",
        "  draft_name = ['delta_rasm', 'theta_rasm', 'alpha_rasm', 'beta_rasm', 'gamma_rasm', 'delta_dasm', 'theta_dasm', 'alpha_dasm', 'beta_dasm', 'gamma_dasm']\n",
        "  for i in draft_name:\n",
        "    channels_name.append(i)\n",
        "  return channels_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB8m5cVgX1KR"
      },
      "outputs": [],
      "source": [
        "eeg_channels = np.array([\"Fp1\", \"AF3\", \"F3\", \"F7\", \"FC5\", \"FC1\", \"C3\", \"T7\", \"CP5\", \"CP1\", \"P3\", \"P7\",\n",
        "                         \"PO3\", \"O1\", \"Oz\", \"Pz\", \"Fp2\", \"AF4\", \"Fz\", \"F4\", \"F8\", \"FC6\", \"FC2\", \"Cz\",\n",
        "                         \"C4\", \"T8\", \"CP6\", \"CP2\", \"P4\", \"P8\", \"PO4\", \"O2\"])\n",
        "subject_names = [\"s01\", \"s02\", \"s03\", \"s04\", \"s05\", \"s06\", \"s07\", \"s08\", \"s09\", \"s10\", \"s11\", \"s12\",\n",
        "                 \"s13\", \"s14\", \"s15\", \"s16\", \"s17\", \"s18\", \"s19\", \"s20\", \"s21\",\n",
        "                \"s22\", \"s23\", \"s24\", \"s25\", \"s26\", \"s27\", \"s28\", \"s29\", \"s30\", \"s31\", \"s32\"]\n",
        "def get_channel_no(channel_name):\n",
        "  channel_no = []\n",
        "  for i in range(0, len(eeg_channels)):\n",
        "    for j in range(0, len(channel_name)):\n",
        "      if(eeg_channels[i] == channel_name[j]):\n",
        "        channel_no.append(i)\n",
        "  return channel_no # is in sorted order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1hdHEVrKYQk"
      },
      "outputs": [],
      "source": [
        "subject_arousal_channels = [['P7', 'CP1', 'P3', 'Fz', 'P8', 'FC2', 'F7', 'C4'], ['FC2'],\n",
        "                            ['AF4', 'FC1', 'O2', 'F3', 'AF3', 'P8', 'T7', 'C4', 'Pz', 'FC5', 'PO3'], ['P8', 'CP2', 'P7', 'Pz', 'P3'], ['Fz', 'P7', 'T7', 'CP6', 'O2', 'Fp1'], \n",
        "                            ['C3', 'AF4', 'AF3', 'P7', 'F3', 'F4', 'CP5', 'T8', 'Oz', 'FC6', 'F7', 'Cz'], ['FC1', 'F3', 'O1', 'FC5', 'P3', 'Fz'], ['T8', 'Fp1', 'FC6', 'Fp2', 'Fz', 'CP2', 'PO3', 'CP1', 'Cz'], ['AF4'],\n",
        "                            ['FC1', 'Fp1', 'CP5', 'P7', 'T8', 'PO4', 'C3', 'PO3', 'F4', 'Cz'], ['T7'], ['FC6', 'CP5', 'F3', 'Cz', 'FC2'], ['PO3', 'P7', 'Fp2'], ['PO3', 'FC2', 'P3', 'AF3', 'CP5', 'Pz', 'F7', 'T7', 'O2', 'F3'],\n",
        "                            ['FC2', 'FC1', 'Fz', 'P3', 'AF4', 'Cz', 'CP1', 'CP2', 'Fp2', 'AF3', 'CP5', 'Pz'], ['O2', 'FC1', 'C3', 'C4', 'Cz'], ['P4', 'CP2', 'FC2', 'Fp1'], ['AF4', 'FC6', 'FC5', 'PO4', 'P3', 'CP1'], \n",
        "                            ['CP5', 'P8', 'C4', 'FC6', 'PO4', 'O2', 'P4', 'CP6', 'P7', 'CP2', 'PO3', 'FC2'], ['F3', 'P7', 'CP6', 'F4', 'FC6', 'F8', 'CP5', 'Fz', 'T7',  'FC5', 'AF3', 'O1', 'CP1', 'Cz', 'F7', 'C3', 'FC1'], \n",
        "                            ['P4', 'Fz'], ['O1', 'CP2', 'FC1', 'CP1', 'CP6', 'F8', 'P8'], ['Fp1', 'F3', 'CP5', 'Cz', 'FC5'],\n",
        "                            ['Pz', 'O2', 'P8', 'CP2', 'O1', 'PO3', 'F8', 'FC1', 'P3', 'CP1', 'Cz', 'T8', 'FC2', 'C3', 'FC5', 'Fz', 'CP5', 'F4', 'PO4', 'F7', 'Oz', 'T7', 'P7', 'AF4', 'Fp2', 'FC6', 'F3', 'P4', 'Fp1', 'AF3', 'C4', 'CP6'],\n",
        "                            ['FC2'], ['AF4', 'FC5', 'CP5', 'Oz', 'FC2', 'C3', 'Cz'], ['PO3', 'Oz', 'AF4', 'CP6', 'F8', 'Cz', 'FC6', 'O1', 'P8', 'F4', 'Fp2'],\n",
        "                            ['PO3', 'F4', 'FC6', 'Oz', 'CP2', 'O2', 'P4', 'CP6'], ['O2', 'CP6', 'Cz', 'PO4', 'O1', 'FC1', 'T8', 'P4', 'F4', 'C3'],\n",
        "                            ['Oz', 'P4', 'CP1', 'P3', 'FC6', 'AF3', 'F3', 'F7', 'CP2', 'Cz', 'CP5', 'C4', 'FC1', 'P8', 'T8', 'T7', 'F8', 'C3', 'O2', 'CP6', 'Fp1', 'AF4', 'O1', 'F4'],\n",
        "                            ['FC2', 'C4', 'PO4', 'P4', 'F4', 'FC1', 'C3', 'CP6', 'AF3', 'F7', 'FC5'], ['Fz', 'Cz', 'P7', 'F4', 'F8', 'C4', 'P4']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d503JOasS8Ks"
      },
      "outputs": [],
      "source": [
        "# data_dwt is from 0.5hz to 60 Hz\n",
        "# Sample rate and desired cutoff frequencies (in Hz).\n",
        "fs_dwt = 128\n",
        "lowcut_dwt = 0.5\n",
        "highcut_dwt = 45\n",
        "T_dwt = 60\n",
        "nsamples_dwt = T_dwt * fs_dwt\n",
        "t_dwt = np.linspace(0, T_dwt, nsamples_dwt, endpoint=False)\n",
        "a_dwt = 0.02\n",
        "f0_dwt = 128\n",
        "def signal_pro_dwt(data):\n",
        "    mean_value = 0\n",
        "    # do the bandpass filter\n",
        "    for i in range(40):\n",
        "        for j in range(32):\n",
        "            data[i][j] = butter_bandpass_filter(data[i][j], lowcut_dwt, highcut_dwt, fs_dwt, order=5)\n",
        "    # creating dummy variable which contains same data information \n",
        "    error_eye =  np.zeros((40,32,7680))\n",
        "    new_data =  np.zeros((40,32,7680))\n",
        "    for i in range(40):\n",
        "        for j in range(32):\n",
        "            for k in range(7680):\n",
        "                error_eye[i][j][k] = data[i][j][k]\n",
        "                new_data[i][j][k] = data[i][j][k]\n",
        "    for i in range(40):\n",
        "        error_eye[i] = eye_movement_artifact(error_eye[i])\n",
        "    for i in range(40):\n",
        "        for j in range(32):\n",
        "            mean_value = np.mean(error_eye[i][j])\n",
        "            for k in range(7680):\n",
        "                if(data[i][j][k] > 0.0):\n",
        "                    # data is positive\n",
        "                    new_data[i][j][k] = data[i][j][k] - abs(mean_value)\n",
        "                else: # data is negative\n",
        "                    new_data[i][j][k] = data[i][j][k] + abs(mean_value)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vsfNHrTHhEG"
      },
      "outputs": [],
      "source": [
        "def main_function(subject, channel_name):\n",
        "  deap_dataset = pickle.load(open('/content/drive/MyDrive/Deap/' + subject + '.dat','rb'),encoding = 'latin1')\n",
        "  labels, data = deap_dataset['labels'], deap_dataset['data']\n",
        "  data = data[0:40 , 0:32 , 384:8064]\n",
        "  filter_data = signal_pro(data)\n",
        "  channel_no = get_channel_no(channel_name)\n",
        "  data_dwt = signal_pro_dwt(data)\n",
        "  features = get_features(data, data_dwt, channel_no)\n",
        "  channels_name = get_channels_name(channel_no)\n",
        "  df = pd.DataFrame(features, columns = channels_name)\n",
        "  mypath = \"/content/drive/MyDrive/Sequential methods for channel selection/our code/\" + subject + \"/\" + subject + \"_arousal.csv\"\n",
        "  df.to_csv(mypath, index = False, encoding = 'utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM71_UPvPVHr",
        "outputId": "99d69c59-4f74-403b-f519-34f336518551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s01\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s02\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s03\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s04\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s05\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s06\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s07\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s08\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n",
            "video:  39\n",
            "257\n",
            "Done:  s09\n",
            "video:  0\n",
            "video:  1\n",
            "video:  2\n",
            "video:  3\n",
            "video:  4\n",
            "video:  5\n",
            "video:  6\n",
            "video:  7\n",
            "video:  8\n",
            "video:  9\n",
            "video:  10\n",
            "video:  11\n",
            "video:  12\n",
            "video:  13\n",
            "video:  14\n",
            "video:  15\n",
            "video:  16\n",
            "video:  17\n",
            "video:  18\n",
            "video:  19\n",
            "video:  20\n",
            "video:  21\n",
            "video:  22\n",
            "video:  23\n",
            "video:  24\n",
            "video:  25\n",
            "video:  26\n",
            "video:  27\n",
            "video:  28\n",
            "video:  29\n",
            "video:  30\n",
            "video:  31\n",
            "video:  32\n",
            "video:  33\n",
            "video:  34\n",
            "video:  35\n",
            "video:  36\n",
            "video:  37\n",
            "video:  38\n"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "for subject in subject_names:\n",
        "  main_function(subject, subject_arousal_channels[counter])\n",
        "  counter =  counter + 1\n",
        "  print(\"Done: \", subject)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of STEP: 3.a. ---Hybrid Feature Extraction for arousal class.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}